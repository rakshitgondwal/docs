{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"K8sGPT Documentation K8sGPT gives Kubernetes SRE superpowers to everyone The documentation provides the following Getting started: Guides to install and use K8sGPT Tutorials: End-to-end tutorials on specific use cases Reference: Specific documentation on the features Explanation: Additional explanations on the design and use of the CLI Documentation enhancements If anything is unclear please create an issue in the docs repository.","title":"Overview"},{"location":"#k8sgpt-documentation","text":"K8sGPT gives Kubernetes SRE superpowers to everyone The documentation provides the following Getting started: Guides to install and use K8sGPT Tutorials: End-to-end tutorials on specific use cases Reference: Specific documentation on the features Explanation: Additional explanations on the design and use of the CLI","title":"K8sGPT Documentation"},{"location":"#documentation-enhancements","text":"If anything is unclear please create an issue in the docs repository.","title":"Documentation enhancements"},{"location":"explanation/","text":"Explanation This section will provide detailed information on different K8sGPT components, including decisions on design architecture development and more","title":"Explanation"},{"location":"explanation/#explanation","text":"This section will provide detailed information on different K8sGPT components, including decisions on design architecture development and more","title":"Explanation"},{"location":"getting-started/getting-started/","text":"Getting Started Guide You can either get started with K8sGPT in your own environment, the details are provided below or you can use our Playground example on Killrcoda . Prerequisites Ensure k8sgpt is installed correctly on your environment by following the installation . You need to be connected to any Kubernetes cluster. Setting up a Kubernetes cluster To give k8sgpt a try, set up a basic Kubernetes cluster, such as KinD or Minikube (if you are not connected to any other cluster). Tip Please only use K8sGPT on environments where you are authorized to modify Kubernetes resources. The KinD documentation provides several installation options to set up a local cluster with two commands. The Minikube documentation covers different Operative Systems and Architectures to set up a local Kubernetes cluster running on a Container or Virtual Machine. Using K8sGPT You can view the different command options through k8sgpt --help Kubernetes debugging powered by AI Usage: k8sgpt [command] Available Commands: analyze This command will find problems within your Kubernetes cluster auth Authenticate with your chosen backend completion Generate the autocompletion script for the specified shell filters Manage filters for analyzing Kubernetes resources generate Generate Key for your chosen backend (opens browser) help Help about any command integration Integrate another tool into K8sGPT serve Runs k8sgpt as a server version Print the version number of k8sgpt Flags: --config string config file (default is $HOME/.k8sgpt.yaml) -h, --help help for k8sgpt --kubeconfig string Path to a kubeconfig. Only required if out-of-cluster. --kubecontext string Kubernetes context to use. Only required if out-of-cluster. Use \"k8sgpt [command] --help\" for more information about a command. Authenticate with OpenAI First, you will need to authenticate with your chosen backend. The backend is the AI provider such as OpenAI's ChatGPT. Ensure that you have created an account. Next, generate a token from the backend: k8sgpt generate This will provide you with a URL to generate a token, follow the URL from the command line to your browser to then generate the token. Copy the token for the next step. Then, authenticate with the following command: k8sgpt auth new This will request the token that has just been generated. Paste the token into the command line. You should then see the following success message: Enter openai Key: openai added to the AI backend provider list Analyze your cluster Ensure that you are connected the correct Kubernetes cluster, for this initial example is preferable to use KinD or Minikube as discussed earlier. kubectl config current-context kubectl get nodes We will new create a new \"broken Pod\", simply create a new YAML file named broken-pod.yml with the following contents: apiVersion: v1 kind: Pod metadata: name: broken-pod namespace: default spec: containers: - name: broken-pod image: nginx:1.a.b.c livenessProbe: httpGet: path: / port: 81 initialDelaySeconds: 3 periodSeconds: 3 You might have noticed, this Pod has a wrong image tag. This is ok for this example, we simply want to have an issue in our cluster. The simply run: kubectl apply -f broken-pod.yml This will create the \"broken Pod\" in the cluster. You can verify this by running: kubectl get pods NAME READY STATUS RESTARTS AGE broken-pod 0/1 ErrImagePull 0 5s Now, you can go ahead and analyse your cluster: k8sgpt analyze Executing this command will generate a list of issues present in your Kubernetes cluster. In the case of our example, a message should be displayed highlighting the problem related to the container image. 0 default/broken-pod(broken-pod) - Error: Back-off pulling image \"nginx:1.a.b.c\" Info To become acquainted with the available flags supported by the analyse command, type k8sgpt analyse -h for more information. This will provide you with a comprehensive list of all the flags that can be utilized. For a more engaging experience and a better understanding of the capabilities of k8sgpt and LLMs (Large Language Models), run the following command: k8sgpt analyse --explain Congratulations! you have successfully created a local kubernetes cluster, deployed a \"broken Pod\" and analyzed it using k8sgpt .","title":"Getting Started Guide"},{"location":"getting-started/getting-started/#getting-started-guide","text":"You can either get started with K8sGPT in your own environment, the details are provided below or you can use our Playground example on Killrcoda .","title":"Getting Started Guide"},{"location":"getting-started/getting-started/#prerequisites","text":"Ensure k8sgpt is installed correctly on your environment by following the installation . You need to be connected to any Kubernetes cluster.","title":"Prerequisites"},{"location":"getting-started/getting-started/#setting-up-a-kubernetes-cluster","text":"To give k8sgpt a try, set up a basic Kubernetes cluster, such as KinD or Minikube (if you are not connected to any other cluster). Tip Please only use K8sGPT on environments where you are authorized to modify Kubernetes resources. The KinD documentation provides several installation options to set up a local cluster with two commands. The Minikube documentation covers different Operative Systems and Architectures to set up a local Kubernetes cluster running on a Container or Virtual Machine.","title":"Setting up a Kubernetes cluster"},{"location":"getting-started/getting-started/#using-k8sgpt","text":"You can view the different command options through k8sgpt --help Kubernetes debugging powered by AI Usage: k8sgpt [command] Available Commands: analyze This command will find problems within your Kubernetes cluster auth Authenticate with your chosen backend completion Generate the autocompletion script for the specified shell filters Manage filters for analyzing Kubernetes resources generate Generate Key for your chosen backend (opens browser) help Help about any command integration Integrate another tool into K8sGPT serve Runs k8sgpt as a server version Print the version number of k8sgpt Flags: --config string config file (default is $HOME/.k8sgpt.yaml) -h, --help help for k8sgpt --kubeconfig string Path to a kubeconfig. Only required if out-of-cluster. --kubecontext string Kubernetes context to use. Only required if out-of-cluster. Use \"k8sgpt [command] --help\" for more information about a command.","title":"Using K8sGPT"},{"location":"getting-started/getting-started/#authenticate-with-openai","text":"First, you will need to authenticate with your chosen backend. The backend is the AI provider such as OpenAI's ChatGPT. Ensure that you have created an account. Next, generate a token from the backend: k8sgpt generate This will provide you with a URL to generate a token, follow the URL from the command line to your browser to then generate the token. Copy the token for the next step. Then, authenticate with the following command: k8sgpt auth new This will request the token that has just been generated. Paste the token into the command line. You should then see the following success message: Enter openai Key: openai added to the AI backend provider list","title":"Authenticate with OpenAI"},{"location":"getting-started/getting-started/#analyze-your-cluster","text":"Ensure that you are connected the correct Kubernetes cluster, for this initial example is preferable to use KinD or Minikube as discussed earlier. kubectl config current-context kubectl get nodes We will new create a new \"broken Pod\", simply create a new YAML file named broken-pod.yml with the following contents: apiVersion: v1 kind: Pod metadata: name: broken-pod namespace: default spec: containers: - name: broken-pod image: nginx:1.a.b.c livenessProbe: httpGet: path: / port: 81 initialDelaySeconds: 3 periodSeconds: 3 You might have noticed, this Pod has a wrong image tag. This is ok for this example, we simply want to have an issue in our cluster. The simply run: kubectl apply -f broken-pod.yml This will create the \"broken Pod\" in the cluster. You can verify this by running: kubectl get pods NAME READY STATUS RESTARTS AGE broken-pod 0/1 ErrImagePull 0 5s Now, you can go ahead and analyse your cluster: k8sgpt analyze Executing this command will generate a list of issues present in your Kubernetes cluster. In the case of our example, a message should be displayed highlighting the problem related to the container image. 0 default/broken-pod(broken-pod) - Error: Back-off pulling image \"nginx:1.a.b.c\" Info To become acquainted with the available flags supported by the analyse command, type k8sgpt analyse -h for more information. This will provide you with a comprehensive list of all the flags that can be utilized. For a more engaging experience and a better understanding of the capabilities of k8sgpt and LLMs (Large Language Models), run the following command: k8sgpt analyse --explain Congratulations! you have successfully created a local kubernetes cluster, deployed a \"broken Pod\" and analyzed it using k8sgpt .","title":"Analyze your cluster"},{"location":"getting-started/in-cluster-operator/","text":"K8sGPT Operator Prerequisites To begin you will require a Kubernetes cluster available and KUBECONFIG set. You will also need to install helm v3. See the Helm documentation for more information . Operator Installation To install the operator, run the following command: helm repo add k8sgpt https://charts.k8sgpt.ai/ helm repo update helm install release k8sgpt/k8sgpt-operator -n k8sgpt-operator-system --create-namespace This will install the Operator into the cluster, which will await a K8sGPT resource before anything happens. Deploying an OpenAI secret Whilst there are multiple backends supported ( OpenAI, Azure OpenAI and Local ), in this example we'll use OpenAI. Whatever backend you are using, you need to make sure to have a secret that works with the backend. For instance, this means you will need to install your OpenAI token as a secret into the cluster: kubectl create secret generic k8sgpt-sample-secret --from-literal=openai-api-key=$OPENAI_TOKEN -n default Deploying a K8sGPT resource To deploy a K8sGPT resource, you will need to create a YAML file with the following contents: kubectl apply -f - << EOF apiVersion: core.k8sgpt.ai/v1alpha1 kind: K8sGPT metadata: name: k8sgpt-sample spec: namespace: default model: gpt-3.5-turbo backend: openai noCache: false version: <VERSION> enableAI: true secret: name: k8sgpt-sample-secret key: openai-api-key EOF Please replace the <VERSION> field with the current release of K8sGPT . At the time of writing this is v0.3.6 . Regarding out of cluster traffic to AI backends In the above example enableAI is set to true . This option allows the cluster deployment to use the backend to filter and improve the responses to the user. Those responses will appear as details within the Result custom resources that are created. The default backend in this example is OpenAI and allows for additional details to be generated and solutions provided for issues. If you wish to disable out-of-cluster communication and any Artificial Intelligence processing through models, simply set enableAI to false . It should also be noted that localai is supported and in-cluster models will be supported in the near future Viewing the results Once the initial scans have completed after several minutes, you will be presented with results custom resources. \u276f kubectl get results -o json | jq . { \"apiVersion\": \"v1\", \"items\": [ { \"apiVersion\": \"core.k8sgpt.ai/v1alpha1\", \"kind\": \"Result\", \"metadata\": { \"creationTimestamp\": \"2023-04-26T09:45:02Z\", \"generation\": 1, \"name\": \"placementoperatorsystemplacementoperatorcontrollermanagermetricsservice\", \"namespace\": \"default\", \"resourceVersion\": \"108371\", \"uid\": \"f0edd4de-92b6-4de2-ac86-5bb2b2da9736\" }, \"spec\": { \"details\": \"The error message means that the service in Kubernetes doesn't have any associated endpoints, which should have been labeled with \\\"control-plane=controller-manager\\\". \\n\\nTo solve this issue, you need to add the \\\"control-plane=controller-manager\\\" label to the endpoint that matches the service. Once the endpoint is labeled correctly, Kubernetes can associate it with the service, and the error should be resolved.\", ...","title":"In-Cluster Operator"},{"location":"getting-started/in-cluster-operator/#k8sgpt-operator","text":"","title":"K8sGPT Operator"},{"location":"getting-started/in-cluster-operator/#prerequisites","text":"To begin you will require a Kubernetes cluster available and KUBECONFIG set. You will also need to install helm v3. See the Helm documentation for more information .","title":"Prerequisites"},{"location":"getting-started/in-cluster-operator/#operator-installation","text":"To install the operator, run the following command: helm repo add k8sgpt https://charts.k8sgpt.ai/ helm repo update helm install release k8sgpt/k8sgpt-operator -n k8sgpt-operator-system --create-namespace This will install the Operator into the cluster, which will await a K8sGPT resource before anything happens.","title":"Operator Installation"},{"location":"getting-started/in-cluster-operator/#deploying-an-openai-secret","text":"Whilst there are multiple backends supported ( OpenAI, Azure OpenAI and Local ), in this example we'll use OpenAI. Whatever backend you are using, you need to make sure to have a secret that works with the backend. For instance, this means you will need to install your OpenAI token as a secret into the cluster: kubectl create secret generic k8sgpt-sample-secret --from-literal=openai-api-key=$OPENAI_TOKEN -n default","title":"Deploying an OpenAI secret"},{"location":"getting-started/in-cluster-operator/#deploying-a-k8sgpt-resource","text":"To deploy a K8sGPT resource, you will need to create a YAML file with the following contents: kubectl apply -f - << EOF apiVersion: core.k8sgpt.ai/v1alpha1 kind: K8sGPT metadata: name: k8sgpt-sample spec: namespace: default model: gpt-3.5-turbo backend: openai noCache: false version: <VERSION> enableAI: true secret: name: k8sgpt-sample-secret key: openai-api-key EOF Please replace the <VERSION> field with the current release of K8sGPT . At the time of writing this is v0.3.6 .","title":"Deploying a K8sGPT resource"},{"location":"getting-started/in-cluster-operator/#regarding-out-of-cluster-traffic-to-ai-backends","text":"In the above example enableAI is set to true . This option allows the cluster deployment to use the backend to filter and improve the responses to the user. Those responses will appear as details within the Result custom resources that are created. The default backend in this example is OpenAI and allows for additional details to be generated and solutions provided for issues. If you wish to disable out-of-cluster communication and any Artificial Intelligence processing through models, simply set enableAI to false . It should also be noted that localai is supported and in-cluster models will be supported in the near future","title":"Regarding out of cluster traffic to AI backends"},{"location":"getting-started/in-cluster-operator/#viewing-the-results","text":"Once the initial scans have completed after several minutes, you will be presented with results custom resources. \u276f kubectl get results -o json | jq . { \"apiVersion\": \"v1\", \"items\": [ { \"apiVersion\": \"core.k8sgpt.ai/v1alpha1\", \"kind\": \"Result\", \"metadata\": { \"creationTimestamp\": \"2023-04-26T09:45:02Z\", \"generation\": 1, \"name\": \"placementoperatorsystemplacementoperatorcontrollermanagermetricsservice\", \"namespace\": \"default\", \"resourceVersion\": \"108371\", \"uid\": \"f0edd4de-92b6-4de2-ac86-5bb2b2da9736\" }, \"spec\": { \"details\": \"The error message means that the service in Kubernetes doesn't have any associated endpoints, which should have been labeled with \\\"control-plane=controller-manager\\\". \\n\\nTo solve this issue, you need to add the \\\"control-plane=controller-manager\\\" label to the endpoint that matches the service. Once the endpoint is labeled correctly, Kubernetes can associate it with the service, and the error should be resolved.\", ...","title":"Viewing the results"},{"location":"getting-started/installation/","text":"Installation This page provides further information on installation guidelines. Linux/Mac via brew Prerequisites Ensure that you have Homebrew installed: Homebrew for Mac Homebrew for Linux Homebrew for Linux also works on WSL Homebrew Install K8sGPT on your machine with the following commands: brew tap k8sgpt-ai/k8sgpt brew install k8sgpt Other Installation Options RPM-based installation (RedHat/CentOS/Fedora) 32 bit: curl -LO https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.2.1/k8sgpt_386.rpm sudo rpm -ivh k8sgpt_386.rpm 64 bit: curl -LO https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.2.1/k8sgpt_amd64.rpm sudo rpm -ivh -i k8sgpt_amd64.rpm DEB-based installation (Ubuntu/Debian) 32 bit: curl -LO https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.2.7/k8sgpt_386.deb sudo dpkg -i k8sgpt_386.deb 64 bit: curl -LO https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.2.7/k8sgpt_amd64.deb sudo dpkg -i k8sgpt_amd64.deb APK-based installation (Alpine) 32 bit: curl -LO https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.2.7/k8sgpt_386.apk apk add k8sgpt_386.apk 64 bit: curl -LO https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.2.1/k8sgpt_amd64.apk apk add k8sgpt_amd64.apk Windows Download the latest Windows binaries of k8sgpt from the Release tab based on your system architecture. Extract the downloaded package to your desired location. Configure the system path variable with the binary location Verify installation Verify that K8sGPT is installed correctly: k8sgpt version k8sgpt version 0.2.7 Common Issues Windows WSL Failing Installation on WSL or Linux (missing gcc) When installing Homebrew on WSL or Linux, you may encounter the following error: ==> Installing k8sgpt from k8sgpt-ai/k8sgpt Error: The following formula cannot be installed from bottle and must be built from source. k8sgpt Install Clang or run brew install gcc. If you install gcc as suggested, the problem will persist. Therefore, you need to install the build-essential package. sudo apt-get update sudo apt-get install build-essential Failing Installation on WSL or Linux (missing gcc) When installing Homebrew on WSL or Linux, you may encounter the following error: ==> Installing k8sgpt from k8sgpt-ai/k8sgpt Error: The following formula cannot be installed from a bottle and must be built from the source. k8sgpt Install Clang or run brew install gcc. If you install gcc as suggested, the problem will persist. Therefore, you need to install the build-essential package. bash sudo apt-get update sudo apt-get install build-essential Running K8sGPT through a container If you are running K8sGPT through a container, the CLI will not be able to open the website for the OpenAI token. You can find the latest container image for K8sGPT in the packages of the GitHub organisation: Link A volume can then be mounted to the image through e.g. Docker Compose . Below is an example: version: '2' services: k8sgpt: image: ghcr.io/k8sgpt-ai/k8sgpt:dev-202304011623 volumes: - /home/$(whoami)/.k8sgpt.yaml:/home/root/.k8sgpt.yaml Installing the K8sGPT Operator Helm Chart K8sGPT can be installed as an Operator inside the cluster. For further information, see the K8sGPT Operator documentation. Upgrading the brew installation To upgrade the K8sGPT brew installation run the following command: brew upgrade k8sgpt","title":"Installation"},{"location":"getting-started/installation/#installation","text":"This page provides further information on installation guidelines.","title":"Installation"},{"location":"getting-started/installation/#linuxmac-via-brew","text":"","title":"Linux/Mac via brew"},{"location":"getting-started/installation/#prerequisites","text":"Ensure that you have Homebrew installed: Homebrew for Mac Homebrew for Linux Homebrew for Linux also works on WSL","title":"Prerequisites"},{"location":"getting-started/installation/#homebrew","text":"Install K8sGPT on your machine with the following commands: brew tap k8sgpt-ai/k8sgpt brew install k8sgpt","title":"Homebrew"},{"location":"getting-started/installation/#other-installation-options","text":"","title":"Other Installation Options"},{"location":"getting-started/installation/#rpm-based-installation-redhatcentosfedora","text":"32 bit: curl -LO https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.2.1/k8sgpt_386.rpm sudo rpm -ivh k8sgpt_386.rpm 64 bit: curl -LO https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.2.1/k8sgpt_amd64.rpm sudo rpm -ivh -i k8sgpt_amd64.rpm","title":"RPM-based installation (RedHat/CentOS/Fedora)"},{"location":"getting-started/installation/#deb-based-installation-ubuntudebian","text":"32 bit: curl -LO https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.2.7/k8sgpt_386.deb sudo dpkg -i k8sgpt_386.deb 64 bit: curl -LO https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.2.7/k8sgpt_amd64.deb sudo dpkg -i k8sgpt_amd64.deb","title":"DEB-based installation (Ubuntu/Debian)"},{"location":"getting-started/installation/#apk-based-installation-alpine","text":"32 bit: curl -LO https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.2.7/k8sgpt_386.apk apk add k8sgpt_386.apk 64 bit: curl -LO https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.2.1/k8sgpt_amd64.apk apk add k8sgpt_amd64.apk","title":"APK-based installation (Alpine)"},{"location":"getting-started/installation/#windows","text":"Download the latest Windows binaries of k8sgpt from the Release tab based on your system architecture. Extract the downloaded package to your desired location. Configure the system path variable with the binary location","title":"Windows"},{"location":"getting-started/installation/#verify-installation","text":"Verify that K8sGPT is installed correctly: k8sgpt version k8sgpt version 0.2.7","title":"Verify installation"},{"location":"getting-started/installation/#common-issues","text":"","title":"Common Issues"},{"location":"getting-started/installation/#windows-wsl","text":"Failing Installation on WSL or Linux (missing gcc) When installing Homebrew on WSL or Linux, you may encounter the following error: ==> Installing k8sgpt from k8sgpt-ai/k8sgpt Error: The following formula cannot be installed from bottle and must be built from source. k8sgpt Install Clang or run brew install gcc. If you install gcc as suggested, the problem will persist. Therefore, you need to install the build-essential package. sudo apt-get update sudo apt-get install build-essential","title":"Windows WSL"},{"location":"getting-started/installation/#failing-installation-on-wsl-or-linux-missing-gcc","text":"When installing Homebrew on WSL or Linux, you may encounter the following error: ==> Installing k8sgpt from k8sgpt-ai/k8sgpt Error: The following formula cannot be installed from a bottle and must be built from the source. k8sgpt Install Clang or run brew install gcc. If you install gcc as suggested, the problem will persist. Therefore, you need to install the build-essential package. bash sudo apt-get update sudo apt-get install build-essential","title":"Failing Installation on WSL or Linux (missing gcc)"},{"location":"getting-started/installation/#running-k8sgpt-through-a-container","text":"If you are running K8sGPT through a container, the CLI will not be able to open the website for the OpenAI token. You can find the latest container image for K8sGPT in the packages of the GitHub organisation: Link A volume can then be mounted to the image through e.g. Docker Compose . Below is an example: version: '2' services: k8sgpt: image: ghcr.io/k8sgpt-ai/k8sgpt:dev-202304011623 volumes: - /home/$(whoami)/.k8sgpt.yaml:/home/root/.k8sgpt.yaml","title":"Running K8sGPT through a container"},{"location":"getting-started/installation/#installing-the-k8sgpt-operator-helm-chart","text":"K8sGPT can be installed as an Operator inside the cluster. For further information, see the K8sGPT Operator documentation.","title":"Installing the K8sGPT Operator Helm Chart"},{"location":"getting-started/installation/#upgrading-the-brew-installation","text":"To upgrade the K8sGPT brew installation run the following command: brew upgrade k8sgpt","title":"Upgrading the brew installation"},{"location":"reference/cli/","text":"CLI Reference This section provides an overview of the different k8sgpt CLI commands. Prerequisites You need to be connected to a Kubernetes cluster, K8sGPT will access it through your kubeconfig. Signed-up to OpenAI ChatGPT Have the K8sGPT CLI installed Commands Run a scan with the default analyzers k8sgpt generate k8sgpt auth new k8sgpt analyze --explain Filter on resource k8sgpt analyze --explain --filter=Service Filter by namespace k8sgpt analyze --explain --filter=Pod --namespace=default Output to JSON k8sgpt analyze --explain --filter=Service --output=json Anonymize during explain k8sgpt analyze --explain --filter=Service --output=json --anonymize Additional commands List configured backends k8sgpt auth list Remove configured backends k8sgpt auth remove --backend $MY_BACKEND List integrations k8sgpt integrations list Activate integrations k8sgpt integrations activate [integration(s)] Use integration k8sgpt analyze --filter=[integration(s)] Deactivate integrations k8sgpt integrations deactivate [integration(s)] Serve mode k8sgpt serve Analysis with serve mode curl -X GET \"http://localhost:8080/analyze?namespace=k8sgpt&explain=false\"","title":"Overview"},{"location":"reference/cli/#cli-reference","text":"This section provides an overview of the different k8sgpt CLI commands. Prerequisites You need to be connected to a Kubernetes cluster, K8sGPT will access it through your kubeconfig. Signed-up to OpenAI ChatGPT Have the K8sGPT CLI installed","title":"CLI Reference"},{"location":"reference/cli/#commands","text":"Run a scan with the default analyzers k8sgpt generate k8sgpt auth new k8sgpt analyze --explain Filter on resource k8sgpt analyze --explain --filter=Service Filter by namespace k8sgpt analyze --explain --filter=Pod --namespace=default Output to JSON k8sgpt analyze --explain --filter=Service --output=json Anonymize during explain k8sgpt analyze --explain --filter=Service --output=json --anonymize","title":"Commands"},{"location":"reference/cli/#additional-commands","text":"List configured backends k8sgpt auth list Remove configured backends k8sgpt auth remove --backend $MY_BACKEND List integrations k8sgpt integrations list Activate integrations k8sgpt integrations activate [integration(s)] Use integration k8sgpt analyze --filter=[integration(s)] Deactivate integrations k8sgpt integrations deactivate [integration(s)] Serve mode k8sgpt serve Analysis with serve mode curl -X GET \"http://localhost:8080/analyze?namespace=k8sgpt&explain=false\"","title":"Additional commands"},{"location":"reference/cli/filters/","text":"Using Integration and Filters in K8sGPT K8sGPT offers integration with other tools. Once an integration is added to K8sGPT, it is possible to use its resources as additional filters. Filters are a way of selecting which resources you wish to be part of your default analysis. Integrations are a way to add in additional resources to the filter list. The first integration that has been added is Trivy. Trivy is an open source, cloud native security scnaner, maintained by Aqua Security. Use the following command to access all K8sGPT CLI options related to integrations: k8sgpt integrations Activating a new integration Prerequisites Connected to a running Kubernetes cluster, any cluster will work for demonstration purposes To list all integrations run the following command: k8sgpt integrations list This will provide you with a list of available integrations. Activate the Trivy integration: k8sgpt integration activate trivy This will install the Trivy Kubernetes Operator into the Kubernetes cluster and make it possible for K8sGPT to interact with the results of the Operator. Once the Trivy Operator is installed inside the cluster, K8sGPT will have access to VulnerabilityReports: k8sgpt filters list Active: > VulnerabilityReport (integration) Unused: > Pod > Deployment > Service > StatefulSet > ReplicaSet > PersistentVolumeClaim > Ingress > CronJob > Node > NetworkPolicy > HorizontalPodAutoScaler > PodDisruptionBudget Using the new filters to analyze your cluster Any of the filters listed in the previous section can be used as part of the k8sgpt analyze command. To use the VulnerabilityReport filter from the Trivy integration, set it through the --filter flag: k8sgpt analyze --filter VulnerabilityReport This command will analyze your cluster Vulnerabilities through K8sGPT. Depnding on the VulnerabilityReports available in your cluster, the result of the report will look different: \u276f k8sgpt analyze --filter VulnerabilityReport 0 demo/nginx-deployment-7bcfc88bbf(Deployment/nginx-deployment) - Error: critical Vulnerability found ID: CVE-2023-23914 (learn more at: https://avd.aquasec.com/nvd/cve-2023-23914) - Error: critical Vulnerability found ID: CVE-2023-27536 (learn more at: https://avd.aquasec.com/nvd/cve-2023-27536) - Error: critical Vulnerability found ID: CVE-2023-23914 (learn more at: https://avd.aquasec.com/nvd/cve-2023-23914) - Error: critical Vulnerability found ID: CVE-2023-27536 (learn more at: https://avd.aquasec.com/nvd/cve-2023-27536) - Error: critical Vulnerability found ID: CVE-2019-8457 (learn more at: https://avd.aquasec.com/nvd/cve-2019-8457) Adding and removing default filters Remove default filters k8sgpt filters add [filter(s)] Simple filter : k8sgpt filters add Service Multiple filters : k8sgpt filters add Ingress,Pod Remove default filters k8sgpt filters remove [filter(s)] Simple filter : k8sgpt filters remove Service Multiple filters : k8sgpt filters remove Ingress,Pod","title":"Integration and Filter"},{"location":"reference/cli/filters/#using-integration-and-filters-in-k8sgpt","text":"K8sGPT offers integration with other tools. Once an integration is added to K8sGPT, it is possible to use its resources as additional filters. Filters are a way of selecting which resources you wish to be part of your default analysis. Integrations are a way to add in additional resources to the filter list. The first integration that has been added is Trivy. Trivy is an open source, cloud native security scnaner, maintained by Aqua Security. Use the following command to access all K8sGPT CLI options related to integrations: k8sgpt integrations","title":"Using Integration and Filters in K8sGPT"},{"location":"reference/cli/filters/#activating-a-new-integration","text":"Prerequisites Connected to a running Kubernetes cluster, any cluster will work for demonstration purposes To list all integrations run the following command: k8sgpt integrations list This will provide you with a list of available integrations. Activate the Trivy integration: k8sgpt integration activate trivy This will install the Trivy Kubernetes Operator into the Kubernetes cluster and make it possible for K8sGPT to interact with the results of the Operator. Once the Trivy Operator is installed inside the cluster, K8sGPT will have access to VulnerabilityReports: k8sgpt filters list Active: > VulnerabilityReport (integration) Unused: > Pod > Deployment > Service > StatefulSet > ReplicaSet > PersistentVolumeClaim > Ingress > CronJob > Node > NetworkPolicy > HorizontalPodAutoScaler > PodDisruptionBudget","title":"Activating a new integration"},{"location":"reference/cli/filters/#using-the-new-filters-to-analyze-your-cluster","text":"Any of the filters listed in the previous section can be used as part of the k8sgpt analyze command. To use the VulnerabilityReport filter from the Trivy integration, set it through the --filter flag: k8sgpt analyze --filter VulnerabilityReport This command will analyze your cluster Vulnerabilities through K8sGPT. Depnding on the VulnerabilityReports available in your cluster, the result of the report will look different: \u276f k8sgpt analyze --filter VulnerabilityReport 0 demo/nginx-deployment-7bcfc88bbf(Deployment/nginx-deployment) - Error: critical Vulnerability found ID: CVE-2023-23914 (learn more at: https://avd.aquasec.com/nvd/cve-2023-23914) - Error: critical Vulnerability found ID: CVE-2023-27536 (learn more at: https://avd.aquasec.com/nvd/cve-2023-27536) - Error: critical Vulnerability found ID: CVE-2023-23914 (learn more at: https://avd.aquasec.com/nvd/cve-2023-23914) - Error: critical Vulnerability found ID: CVE-2023-27536 (learn more at: https://avd.aquasec.com/nvd/cve-2023-27536) - Error: critical Vulnerability found ID: CVE-2019-8457 (learn more at: https://avd.aquasec.com/nvd/cve-2019-8457)","title":"Using the new filters to analyze your cluster"},{"location":"reference/cli/filters/#adding-and-removing-default-filters","text":"Remove default filters k8sgpt filters add [filter(s)] Simple filter : k8sgpt filters add Service Multiple filters : k8sgpt filters add Ingress,Pod Remove default filters k8sgpt filters remove [filter(s)] Simple filter : k8sgpt filters remove Service Multiple filters : k8sgpt filters remove Ingress,Pod","title":"Adding and removing default filters"},{"location":"reference/cli/serve-mode/","text":"K8sGPT Serve mode Prerequisites Have grpcurl installed Run k8sgpt serve mode $ k8sgpt serve {\"level\":\"info\",\"ts\":1684309627.113916,\"caller\":\"server/server.go:83\",\"msg\":\"binding metrics to 8081\"} {\"level\":\"info\",\"ts\":1684309627.114198,\"caller\":\"server/server.go:68\",\"msg\":\"binding api to 8080\"} This command starts two servers: The health server runs on port 8081 by default and serves metrics and health endpoints. The API server runs on port 8080 (gRPC) by default and serves the analysis handler. For more details about the gRPC implementation, refer to this link . Analyze your cluster with grpcurl Make sure you are connected to a Kubernetes cluster: kubectl get nodes Next, run the following command: grpcurl -plaintext localhost:8080 schema.v1.Server/Analyze This command provides a list of issues in your Kubernetes cluster. Analyze with parameters You can specify parameters using the following command: grpcurl -plaintext -d '{\"explain\": false, \"filters\": [\"Ingress\"], \"namespace\": \"k8sgpt\"}' localhost:8080 schema.v1.Server/Analyze In this example, the analyzer will only consider the k8sgpt namespace without AI explanation and only focus on the Ingress filter.","title":"Serve mode"},{"location":"reference/cli/serve-mode/#k8sgpt-serve-mode","text":"","title":"K8sGPT Serve mode"},{"location":"reference/cli/serve-mode/#prerequisites","text":"Have grpcurl installed","title":"Prerequisites"},{"location":"reference/cli/serve-mode/#run-k8sgpt-serve-mode","text":"$ k8sgpt serve {\"level\":\"info\",\"ts\":1684309627.113916,\"caller\":\"server/server.go:83\",\"msg\":\"binding metrics to 8081\"} {\"level\":\"info\",\"ts\":1684309627.114198,\"caller\":\"server/server.go:68\",\"msg\":\"binding api to 8080\"} This command starts two servers: The health server runs on port 8081 by default and serves metrics and health endpoints. The API server runs on port 8080 (gRPC) by default and serves the analysis handler. For more details about the gRPC implementation, refer to this link .","title":"Run k8sgpt serve mode"},{"location":"reference/cli/serve-mode/#analyze-your-cluster-with-grpcurl","text":"Make sure you are connected to a Kubernetes cluster: kubectl get nodes Next, run the following command: grpcurl -plaintext localhost:8080 schema.v1.Server/Analyze This command provides a list of issues in your Kubernetes cluster.","title":"Analyze your cluster with grpcurl"},{"location":"reference/cli/serve-mode/#analyze-with-parameters","text":"You can specify parameters using the following command: grpcurl -plaintext -d '{\"explain\": false, \"filters\": [\"Ingress\"], \"namespace\": \"k8sgpt\"}' localhost:8080 schema.v1.Server/Analyze In this example, the analyzer will only consider the k8sgpt namespace without AI explanation and only focus on the Ingress filter.","title":"Analyze with parameters"},{"location":"reference/guidelines/community/","text":"Community Information All community related information are kept in a separate repository from the docs. Link to the repository: k8sgpt-ai/community There you will find information on The Charter Adopters List Code of Conduct Community Members Subprojects and much more.","title":"Community"},{"location":"reference/guidelines/community/#community-information","text":"All community related information are kept in a separate repository from the docs. Link to the repository: k8sgpt-ai/community There you will find information on The Charter Adopters List Code of Conduct Community Members Subprojects and much more.","title":"Community Information"},{"location":"reference/guidelines/guidelines/","text":"Contributing Guidelines Contributing to the Documentation This documentation follows the Diataxis framework. If you are proposing completely new content to the documentation, please familiarise yourself with the framework first. The documentation is created with mkdocs , specifically the Material for MkDocs theme . Contributing projects in the K8sGPT organisation All project in the K8sGPT organisation follow our contributing guidelines.","title":"Guidelines"},{"location":"reference/guidelines/guidelines/#contributing-guidelines","text":"","title":"Contributing Guidelines"},{"location":"reference/guidelines/guidelines/#contributing-to-the-documentation","text":"This documentation follows the Diataxis framework. If you are proposing completely new content to the documentation, please familiarise yourself with the framework first. The documentation is created with mkdocs , specifically the Material for MkDocs theme .","title":"Contributing to the Documentation"},{"location":"reference/guidelines/guidelines/#contributing-projects-in-the-k8sgpt-organisation","text":"All project in the K8sGPT organisation follow our contributing guidelines.","title":"Contributing projects in the K8sGPT organisation"},{"location":"reference/guidelines/privacy/","text":"Privacy K8sGPT is a privacy-first tool and believe transparency is key for you to understand how we use your data. We have created this page to help you understand how we collect, use, share and protect your data. Data we collect K8sGPT will collect data from Analyzers and either display it directly to you or with the --explain flag it will send it to the selected AI backend. The type of data collected depends on the Analyzer you are using. For example, the k8sgpt analyze pod command will collect the following data: - Container status message - Pod name - Pod namespace - Event message Data we share As mentioned, K8sGPT will share data with the selected AI backend only when you choose --explain and auth against that backend. The data shared will be the same as the data collected. To learn more about the privacy policy of our default AI backend OpenAI please visit their privacy policy . Data we protect When you are sending data through the --explain option, there is the capability of anonymising some of that data. This is done by using the --anonymise flag. In the example of the Deployment Analyzer, this will obfusicate the following data: Deployment name Deployment namespace Data we don't collect Logs API Server data other than the primitives used within our Analyzers. Contact If you have any questions about our privacy policy, please contact us .","title":"Privacy"},{"location":"reference/guidelines/privacy/#privacy","text":"K8sGPT is a privacy-first tool and believe transparency is key for you to understand how we use your data. We have created this page to help you understand how we collect, use, share and protect your data.","title":"Privacy"},{"location":"reference/guidelines/privacy/#data-we-collect","text":"K8sGPT will collect data from Analyzers and either display it directly to you or with the --explain flag it will send it to the selected AI backend. The type of data collected depends on the Analyzer you are using. For example, the k8sgpt analyze pod command will collect the following data: - Container status message - Pod name - Pod namespace - Event message","title":"Data we collect"},{"location":"reference/guidelines/privacy/#data-we-share","text":"As mentioned, K8sGPT will share data with the selected AI backend only when you choose --explain and auth against that backend. The data shared will be the same as the data collected. To learn more about the privacy policy of our default AI backend OpenAI please visit their privacy policy .","title":"Data we share"},{"location":"reference/guidelines/privacy/#data-we-protect","text":"When you are sending data through the --explain option, there is the capability of anonymising some of that data. This is done by using the --anonymise flag. In the example of the Deployment Analyzer, this will obfusicate the following data: Deployment name Deployment namespace","title":"Data we protect"},{"location":"reference/guidelines/privacy/#data-we-dont-collect","text":"Logs API Server data other than the primitives used within our Analyzers.","title":"Data we don't collect"},{"location":"reference/guidelines/privacy/#contact","text":"If you have any questions about our privacy policy, please contact us .","title":"Contact"},{"location":"reference/operator/overview/","text":"K8sGPT Operator K8sGPT can run as a Kubernetes Operator inside the cluster. The scan results are provided as Kubernetes YAML manifests. This section will only detail how to configure the operator. For installatio instrusction, please see the getting-started section. Architecture The diagram below showcases the different components that the K8sGPT Operator installs and manages: Customising the Operator As with other Helm Charts, the K8sGPT Operator can be customised by modifying the values.yaml file. The following fields can be customised in the Helm Chart Deployment: Parameter Description Default serviceMonitor.enabled Enable Prometheus Operator ServiceMonitor false controllerManager.manager.image.repository Image repository k8sgpt/k8sgpt-operator controllerManager.manager.image.pullPolicy Image pull policy IfNotPresent controllerManager.manager.imagePullSecrets Image pull secrets [] For example: In-cluster metrics It is possible to enable metrics of the operator so that they can be scraped through Prometheus. This is the configuration required in the values.yaml manifest: serviceMonitor: enabled: true The new values.yaml manifest can then be provided upon installing the Operator inside the cluster: helm install release k8sgpt/k8sgpt-operator --values values.yaml","title":"Overview"},{"location":"reference/operator/overview/#k8sgpt-operator","text":"K8sGPT can run as a Kubernetes Operator inside the cluster. The scan results are provided as Kubernetes YAML manifests. This section will only detail how to configure the operator. For installatio instrusction, please see the getting-started section.","title":"K8sGPT Operator"},{"location":"reference/operator/overview/#architecture","text":"The diagram below showcases the different components that the K8sGPT Operator installs and manages:","title":"Architecture"},{"location":"reference/operator/overview/#customising-the-operator","text":"As with other Helm Charts, the K8sGPT Operator can be customised by modifying the values.yaml file. The following fields can be customised in the Helm Chart Deployment: Parameter Description Default serviceMonitor.enabled Enable Prometheus Operator ServiceMonitor false controllerManager.manager.image.repository Image repository k8sgpt/k8sgpt-operator controllerManager.manager.image.pullPolicy Image pull policy IfNotPresent controllerManager.manager.imagePullSecrets Image pull secrets []","title":"Customising the Operator"},{"location":"reference/operator/overview/#for-example-in-cluster-metrics","text":"It is possible to enable metrics of the operator so that they can be scraped through Prometheus. This is the configuration required in the values.yaml manifest: serviceMonitor: enabled: true The new values.yaml manifest can then be provided upon installing the Operator inside the cluster: helm install release k8sgpt/k8sgpt-operator --values values.yaml","title":"For example: In-cluster metrics"},{"location":"reference/providers/backend/","text":"K8sGPT AI Backends A Backend or a Provider is a service that provides access to the AI language model. There are many different backends available for K8sGPT. Each backend has its own strengths and weaknesses, so it is important to choose the one that is right for your needs. Currently we have a total of 4 backends available: - OpenAI - Azure OpenAI - LocalAI - FakeAI OpenAI OpenAI is the default backend for K8sGPT. We recommend using OpenAI first if you are new to K8sGPT. OpenAI comes with the access to powerful language models such as GPT-3.5-Turbo, GPT-4. If you are looking for a powerful and easy-to-use language modeling service, OpenAI is a great option. To use OpenAI you'll need an OpenAI token for authentication purposes. To generate a token use: k8sgpt generate To set the token in K8sGPT, use the following command: k8sgpt auth new Run the following command to analyze issues within your cluster using OpenAI: k8sgpt analyze Azure OpenAI Azure OpenAI Provider provides REST API access to OpenAI's powerful language models. It gives the users an advanced language AI with powerful models with the security and enterprise promise of Azure. The Azure OpenAI Provider requires a deployment as a prerequisite. You can visit their documentation to create your own. To authenticate with k8sgpt, you would require an Azure OpenAI endpoint of your tenant https://your Azure OpenAI Endpoint ,the API key to access your deployment, the Deployment name of your model and the model name itself. Run the following command to authenticate with Azure OpenAI: k8sgpt auth --backend azureopenai --baseurl https://<your Azure OpenAI endpoint> --engine <deployment_name> --model <model_name> Now you are ready to analyze with the Azure OpenAI backend: k8sgpt analyze --explain --backend azureopenai LocalAI LocalAI is a local model, which is an OpenAI compatible API. It uses llama.cpp and ggml to run inference on consumer-grade hardware. Models supported by LocalAI for instance are Vicuna, Alpaca, LLaMA, Cerebras, GPT4ALL, GPT4ALL-J and koala. To run To run local inference, you need to download the models first, for instance you can find ggml compatible models in huggingface.com (for example vicuna, alpaca and koala). To start the API server, follow the instruction in LocalAI . Authenticate K8sGPT with LocalAI: k8sgpt auth new --backend localai --model <model_name> --baseurl http://localhost:8080/v1 Analyze with a LocalAI backend: ``` k8sgpt analyze --explain --backend localai ``` FakeAI FakeAI or the NoOpAiProvider might be useful in situations where you need to test a new feature or simulate the behaviour of an AI based-system without actually invoking it. It can help you with local development, testing and troubleshooting. The NoOpAiProvider does not actually perfornm any AI-based operations but simulates them by echoing the input given as a problem. Follow the steps outlined below to learn how to utilize the NoOpAiProvider: Authorize k8sgpt with noopai or noop as the Backend Provider: k8sgpt auth -b noopai For the auth token, you can leave it blank as the NoOpAiProvider is configured to work fine with or without any token. Use the analyze and explain command to check for errors in your kubernetes cluster and the NoOpAiProvider should return the error as the solution itself: k8sgpt analyze --explain","title":"Overview"},{"location":"reference/providers/backend/#k8sgpt-ai-backends","text":"A Backend or a Provider is a service that provides access to the AI language model. There are many different backends available for K8sGPT. Each backend has its own strengths and weaknesses, so it is important to choose the one that is right for your needs. Currently we have a total of 4 backends available: - OpenAI - Azure OpenAI - LocalAI - FakeAI","title":"K8sGPT AI Backends"},{"location":"reference/providers/backend/#openai","text":"OpenAI is the default backend for K8sGPT. We recommend using OpenAI first if you are new to K8sGPT. OpenAI comes with the access to powerful language models such as GPT-3.5-Turbo, GPT-4. If you are looking for a powerful and easy-to-use language modeling service, OpenAI is a great option. To use OpenAI you'll need an OpenAI token for authentication purposes. To generate a token use: k8sgpt generate To set the token in K8sGPT, use the following command: k8sgpt auth new Run the following command to analyze issues within your cluster using OpenAI: k8sgpt analyze","title":"OpenAI"},{"location":"reference/providers/backend/#azure-openai","text":"Azure OpenAI Provider provides REST API access to OpenAI's powerful language models. It gives the users an advanced language AI with powerful models with the security and enterprise promise of Azure. The Azure OpenAI Provider requires a deployment as a prerequisite. You can visit their documentation to create your own. To authenticate with k8sgpt, you would require an Azure OpenAI endpoint of your tenant https://your Azure OpenAI Endpoint ,the API key to access your deployment, the Deployment name of your model and the model name itself. Run the following command to authenticate with Azure OpenAI: k8sgpt auth --backend azureopenai --baseurl https://<your Azure OpenAI endpoint> --engine <deployment_name> --model <model_name> Now you are ready to analyze with the Azure OpenAI backend: k8sgpt analyze --explain --backend azureopenai","title":"Azure OpenAI"},{"location":"reference/providers/backend/#localai","text":"LocalAI is a local model, which is an OpenAI compatible API. It uses llama.cpp and ggml to run inference on consumer-grade hardware. Models supported by LocalAI for instance are Vicuna, Alpaca, LLaMA, Cerebras, GPT4ALL, GPT4ALL-J and koala. To run To run local inference, you need to download the models first, for instance you can find ggml compatible models in huggingface.com (for example vicuna, alpaca and koala). To start the API server, follow the instruction in LocalAI . Authenticate K8sGPT with LocalAI: k8sgpt auth new --backend localai --model <model_name> --baseurl http://localhost:8080/v1 Analyze with a LocalAI backend: ``` k8sgpt analyze --explain --backend localai ```","title":"LocalAI"},{"location":"reference/providers/backend/#fakeai","text":"FakeAI or the NoOpAiProvider might be useful in situations where you need to test a new feature or simulate the behaviour of an AI based-system without actually invoking it. It can help you with local development, testing and troubleshooting. The NoOpAiProvider does not actually perfornm any AI-based operations but simulates them by echoing the input given as a problem. Follow the steps outlined below to learn how to utilize the NoOpAiProvider: Authorize k8sgpt with noopai or noop as the Backend Provider: k8sgpt auth -b noopai For the auth token, you can leave it blank as the NoOpAiProvider is configured to work fine with or without any token. Use the analyze and explain command to check for errors in your kubernetes cluster and the NoOpAiProvider should return the error as the solution itself: k8sgpt analyze --explain","title":"FakeAI"},{"location":"tutorials/","text":"Tutorials This section provides end-to-end tutorials on specific use cases a collection of user and contributor created content","title":"Overview"},{"location":"tutorials/#tutorials","text":"This section provides end-to-end tutorials on specific use cases a collection of user and contributor created content","title":"Tutorials"},{"location":"tutorials/playground/","text":"K8sGPT Playground If you want to try out K8sGPT, we highly suggest you to follow this Killrcoda example: Link: K8sGPT CLI Tutorial This tutorials covers: Run a simple analysis and explore possible options Discover how AI works Explanation Stay on the down-low with the anonymize option (because we don't want any trouble with the feds) Filter resources like a boss Use Integrations","title":"K8sGPT Playground"},{"location":"tutorials/playground/#k8sgpt-playground","text":"If you want to try out K8sGPT, we highly suggest you to follow this Killrcoda example: Link: K8sGPT CLI Tutorial This tutorials covers: Run a simple analysis and explore possible options Discover how AI works Explanation Stay on the down-low with the anonymize option (because we don't want any trouble with the feds) Filter resources like a boss Use Integrations","title":"K8sGPT Playground"},{"location":"tutorials/content-collection/content-collection/","text":"Content Collection This section provides a collection of vidoes, blog posts and more on K8sGPT, posted on external sites. Blogs Have a look at the K8sGPT blog on the website. Additionally, here are several blogs created by the community: K8sGPT: The Ultimate Tool for Kubernetes Scanning by Rakshit Gondwal K8sGPT + LocalAI: Unlock Kubernetes superpowers for free! by Tyler Gillson K8sGPT: Simplifying Kubernetes Diagnostics with Natural Language Processing by Karan Singh Kubernetes + ChatGPT = K8sGpt by Vijul Patel ChatGPT for your Kubernetes Cluster \u2014 k8sgpt by Renjith Ravindranathan Using the Trivy K8sGPT plugin by Renjith Ravindranathan Videos Kubernetes + OpenAI = K8sGPT, giving you AI superpowers! k8sgpt Getting Started (2023) Debugging Kubernetes with AI: k8sGPT || AI-Powered Debugging for Kubernetes Contributing If you have created any content around K8sGPT, then please add it to this collection.","title":"Content Collection"},{"location":"tutorials/content-collection/content-collection/#content-collection","text":"This section provides a collection of vidoes, blog posts and more on K8sGPT, posted on external sites.","title":"Content Collection"},{"location":"tutorials/content-collection/content-collection/#blogs","text":"Have a look at the K8sGPT blog on the website. Additionally, here are several blogs created by the community: K8sGPT: The Ultimate Tool for Kubernetes Scanning by Rakshit Gondwal K8sGPT + LocalAI: Unlock Kubernetes superpowers for free! by Tyler Gillson K8sGPT: Simplifying Kubernetes Diagnostics with Natural Language Processing by Karan Singh Kubernetes + ChatGPT = K8sGpt by Vijul Patel ChatGPT for your Kubernetes Cluster \u2014 k8sgpt by Renjith Ravindranathan Using the Trivy K8sGPT plugin by Renjith Ravindranathan","title":"Blogs"},{"location":"tutorials/content-collection/content-collection/#videos","text":"Kubernetes + OpenAI = K8sGPT, giving you AI superpowers! k8sgpt Getting Started (2023) Debugging Kubernetes with AI: k8sGPT || AI-Powered Debugging for Kubernetes","title":"Videos"},{"location":"tutorials/content-collection/content-collection/#contributing","text":"If you have created any content around K8sGPT, then please add it to this collection.","title":"Contributing"}]}